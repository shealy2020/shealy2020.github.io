<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="pretty-atom-feed.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>TW x AI</title>
  <subtitle>Navigating the intersections of technical writing and AI</subtitle>
  <link href="https://shealy2020.github.io/feed/feed.xml" rel="self" />
  <link href="https://shealy2020.github.io/" />
  <updated>2025-10-27T00:00:00Z</updated>
  <id>https://shealy2020.github.io/</id>
  <author>
    <name>Sean Healy</name>
  </author>
  <entry>
    <title>Setting up Local RAG Pipeline in Docker</title>
    <link href="https://shealy2020.github.io/posts/local-rag-in-docker/" />
    <updated>2025-10-27T00:00:00Z</updated>
    <id>https://shealy2020.github.io/posts/local-rag-in-docker/</id>
    <content type="html">&lt;p&gt;This project is a prototype of a local Retrieval-Augmented Generation (RAG) system, exploring how RAG pipelines work. It was a useful step before scaling or adapting it  to production environments, where proprietary information is behind a firewall. Everything runs entirely on a local pc and requires no cloud services or external data transfers. I used open-source tools only — Python, Chroma, Sentence Transformers, and Ollama.&lt;/p&gt;
&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Windows 11 with WSL 2&lt;/li&gt;
&lt;li&gt;Docker Desktop installed with WSL enabled&lt;/li&gt;
&lt;li&gt;Python installed&lt;/li&gt;
&lt;li&gt;At least 4GB RAM available&lt;/li&gt;
&lt;li&gt;At least 5GB disk space&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;rag-pipeline-components&quot;&gt;RAG Pipeline Components&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LLM&lt;/strong&gt;: Gemma 2B (via Ollama)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embeddings&lt;/strong&gt;: all-MiniLM-L6-v2&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vector Store&lt;/strong&gt;: ChromaDB&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interface&lt;/strong&gt;: Flask + HTML&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supported Formats&lt;/strong&gt;: Markdown (.md), HTML (.html, .htm)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;privacy-and-security&quot;&gt;Privacy &amp;amp; Security&lt;/h2&gt;
&lt;p&gt;This pipeline is a prototype. It provides basic safeguards but is not production ready.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All data stays on your machine&lt;/li&gt;
&lt;li&gt;No external API calls after setup&lt;/li&gt;
&lt;li&gt;No telemetry or analytics&lt;/li&gt;
&lt;li&gt;Can run with complete network isolation&lt;/li&gt;
&lt;li&gt;Docker container isolation&lt;/li&gt;
&lt;li&gt;API Key for extra layer of security&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;summary-of-python-scripts&quot;&gt;Summary of Python Scripts&lt;/h2&gt;
&lt;h3 id=&quot;1-config-py-configuration-management&quot;&gt;&lt;strong&gt;1. config.py&lt;/strong&gt; - Configuration Management&lt;/h3&gt;
&lt;p&gt;Central configuration file that defines all system settings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Paths&lt;/strong&gt;: Input documents, ChromaDB storage, model cache locations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Models&lt;/strong&gt;: Uses &lt;code&gt;sentence-transformers/all-MiniLM-L6-v2&lt;/code&gt; for embeddings and &lt;code&gt;gemma:2b&lt;/code&gt; via Ollama for text generation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chunking&lt;/strong&gt;: Documents split into 512-character chunks with 50-character overlap&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Retrieval&lt;/strong&gt;: Retrieves top 5 most similar chunks with 0.7 similarity threshold&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Flask runs on port 5000&lt;/li&gt;
&lt;li&gt;Uses environment variables for flexibility in deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;2-ingest-py-document-processing-and-indexing&quot;&gt;&lt;strong&gt;2. ingest.py&lt;/strong&gt; - Document Processing &amp;amp; Indexing&lt;/h3&gt;
&lt;p&gt;Handles the ingestion pipeline that prepares documents for retrieval:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DocumentProcessor class:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reads Markdown and HTML files&lt;/li&gt;
&lt;li&gt;Converts them to plain text (strips formatting, scripts, styles)&lt;/li&gt;
&lt;li&gt;Splits text into overlapping chunks for better context preservation&lt;/li&gt;
&lt;li&gt;Creates unique IDs for each chunk using MD5 hashing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;VectorStore class:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manages ChromaDB persistent storage&lt;/li&gt;
&lt;li&gt;Generates embeddings using SentenceTransformer&lt;/li&gt;
&lt;li&gt;Stores document chunks with their vector embeddings&lt;/li&gt;
&lt;li&gt;Provides collection statistics and reset functionality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Main workflow:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scans input directory for &lt;code&gt;.md&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt; files&lt;/li&gt;
&lt;li&gt;Processes each file into chunks&lt;/li&gt;
&lt;li&gt;Generates embeddings in batches (100 at a time)&lt;/li&gt;
&lt;li&gt;Stores everything in ChromaDB for efficient similarity search&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;3-query-py-query-processing-engine&quot;&gt;&lt;strong&gt;3. query.py&lt;/strong&gt; - Query Processing Engine&lt;/h3&gt;
&lt;p&gt;The core RAG logic that answers user questions:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RAGEngine class:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Retrieval&lt;/strong&gt;: Converts user questions to embeddings and finds similar document chunks using vector search&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context Building&lt;/strong&gt;: Assembles retrieved chunks with source attribution&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Response Generation&lt;/strong&gt;: Uses Ollama LLM with a structured prompt that instructs it to answer based only on provided context&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Error Handling&lt;/strong&gt;: Gracefully handles missing collections or empty databases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Query flow:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embed the user&#39;s question&lt;/li&gt;
&lt;li&gt;Find top-K most similar document chunks&lt;/li&gt;
&lt;li&gt;Build context from retrieved chunks&lt;/li&gt;
&lt;li&gt;Send context + question to LLM&lt;/li&gt;
&lt;li&gt;Return answer with source citations&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&quot;4-server-py-web-api-server&quot;&gt;&lt;strong&gt;4. server.py&lt;/strong&gt; - Web API Server&lt;/h3&gt;
&lt;p&gt;Flask-based web server that exposes the RAG system via HTTP:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API Key Authentication&lt;/strong&gt;: Optional security via &lt;code&gt;X-API-Key&lt;/code&gt; header (controlled by &lt;code&gt;RAG_API_KEY&lt;/code&gt; environment variable)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Three endpoints:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET /&lt;/code&gt; - Serves the web interface&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GET /api/status&lt;/code&gt; - Returns system health (document count, available files, readiness)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;POST /api/query&lt;/code&gt; - Processes questions and returns AI-generated answers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lazy Loading&lt;/strong&gt;: RAG engine initialized on first query for faster startup&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Error Handling&lt;/strong&gt;: Comprehensive error responses with appropriate HTTP status codes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Authentication decorator&lt;/strong&gt; ensures protected endpoints require valid API keys when enabled.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;system-architecture&quot;&gt;System Architecture&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;User Question → Flask Server → RAG Engine → ChromaDB (Vector Search)
                                    ↓
                            Retrieved Chunks
                                    ↓
                            Ollama LLM (gemma:2b)
                                    ↓
                            Answer + Sources → User
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;installation-and-setup&quot;&gt;Installation &amp;amp; Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Start Docker Desktop in Windows.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start WSL. A Bash terminal opens in your home directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy &lt;em&gt;rag-local&lt;/em&gt; directory to your home directory.&lt;/p&gt;
&lt;p&gt;Project Structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rag-local/
├── Dockerfile              # Container definition
├── docker-compose.yml      # Docker Compose configuration
├── requirements.txt        # Python dependencies
├── README.md              # This file
├── app/
│   ├── config.py          # Configuration settings
│   ├── ingest.py          # Document ingestion
│   ├── query.py           # RAG query engine
│   ├── server.py          # Flask web server
│   ├── templates/
│   │   └── index.html     # Web interface
│   └── static/
│       └── style.css      # CSS styling
└── data/
 ├── input/             # Your documents (add files here)
 ├── chroma_db/         # Vector database (auto-created)
 └── models/            # Cached ML models (auto-created)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The project contains a sample &lt;code&gt;.md&lt;/code&gt; and &lt;code&gt;.html&lt;/code&gt; file in the &lt;em&gt;data/input&lt;/em&gt; directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generate an API Key.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&quot;language-bash&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;openssl rand &lt;span class=&quot;token parameter variable&quot;&gt;-base64&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;32&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This key will add a minimal security layer.&lt;/p&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;
&lt;p&gt;Copy the API Key. You will need it for the steps that follow. (Also, retain it for later use.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Open &lt;em&gt;docker-compose.yml&lt;/em&gt; and find the following line:
&lt;em&gt;RAG_API_KEY=change-this-to-a-strong-random-key&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace &amp;quot;change-this-to-a-strong-random-key&amp;quot; with your API key.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the project directory.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&quot;language-bash&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token builtin class-name&quot;&gt;cd&lt;/span&gt; ~/rag-local&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&quot;9&quot;&gt;
&lt;li&gt;Build the Docker image.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&quot;language-bash&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;docker-compose&lt;/span&gt; build&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;​	This may take 10-20 minutes as it downloads all required components.&lt;/p&gt;
&lt;ol start=&quot;10&quot;&gt;
&lt;li&gt;Start the container.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&quot;language-bash&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token function&quot;&gt;docker-compose&lt;/span&gt; up &lt;span class=&quot;token parameter variable&quot;&gt;-d&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;​	The first startup after a build may take 5-10 minutes as it downloads the Gemma 2B model (~1.7GB).&lt;/p&gt;
&lt;ol start=&quot;11&quot;&gt;
&lt;li&gt;Open a browser and enter:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;http://localhost:5000
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&quot;12&quot;&gt;
&lt;li&gt;
&lt;p&gt;At the prompt, enter the API key you copied earlier, then click &lt;strong&gt;OK&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The RAG interface opens (Flask + HTML) where you can query against the Markdown and HTML input files.&lt;/p&gt;
&lt;p&gt;The interface also provides commands to add and update input files.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;useful-docker-commands&quot;&gt;Useful Docker Commands&lt;/h2&gt;
&lt;pre class=&quot;language-bash&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# Start the container&lt;/span&gt;
&lt;span class=&quot;token function&quot;&gt;docker-compose&lt;/span&gt; up

&lt;span class=&quot;token comment&quot;&gt;# Start in background (-d = detached mode)&lt;/span&gt;
&lt;span class=&quot;token function&quot;&gt;docker-compose&lt;/span&gt; up &lt;span class=&quot;token parameter variable&quot;&gt;-d&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# Stop the container&lt;/span&gt;
&lt;span class=&quot;token function&quot;&gt;docker-compose&lt;/span&gt; down

&lt;span class=&quot;token comment&quot;&gt;# View logs (-f = follow)&lt;/span&gt;
&lt;span class=&quot;token function&quot;&gt;docker-compose&lt;/span&gt; logs &lt;span class=&quot;token parameter variable&quot;&gt;-f&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# Rebuild after code changes&lt;/span&gt;
&lt;span class=&quot;token function&quot;&gt;docker-compose&lt;/span&gt; build --no-cache
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;troubleshooting&quot;&gt;Troubleshooting&lt;/h2&gt;
&lt;h3 id=&quot;container-wont-start&quot;&gt;Container won&#39;t start&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Check to verify that Docker Desktop is running&lt;/li&gt;
&lt;li&gt;In Docker Desktop, go to &lt;strong&gt;Settings&lt;/strong&gt; &amp;gt; &lt;strong&gt;General&lt;/strong&gt;. Verify that &lt;em&gt;Use the WSL 2 based engine&lt;/em&gt; is enabled&lt;/li&gt;
&lt;li&gt;Check port 5000 is not in use: &lt;code&gt;netstat -an | grep 5000&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;View logs: &lt;code&gt;docker-compose logs&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;ollama-errors&quot;&gt;Ollama errors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Wait longer on first startup (model download takes time)&lt;/li&gt;
&lt;li&gt;Check container logs: &lt;code&gt;docker-compose logs | grep ollama&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Restart container: &lt;code&gt;docker-compose restart&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;out-of-memory&quot;&gt;Out of memory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Reduce &lt;code&gt;TOP_K&lt;/code&gt; in &lt;code&gt;app/config.py&lt;/code&gt; (default: 5)&lt;/li&gt;
&lt;li&gt;Reduce &lt;code&gt;CHUNK_SIZE&lt;/code&gt; in &lt;code&gt;app/config.py&lt;/code&gt; (default: 512)&lt;/li&gt;
&lt;li&gt;Close other applications&lt;/li&gt;
&lt;li&gt;Restart Docker Desktop&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;slow-responses&quot;&gt;Slow responses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This is normal for CPU-only inference with limited pc resources&lt;/li&gt;
&lt;li&gt;Gemma 2B on CPU typically takes 10-30 seconds per response&lt;/li&gt;
&lt;li&gt;Consider reducing the amount of context retrieved (TOP_K)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;configuration&quot;&gt;Configuration&lt;/h2&gt;
&lt;p&gt;Edit &lt;code&gt;app/config.py&lt;/code&gt; to customize:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;CHUNK_SIZE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;512&lt;/span&gt;          &lt;span class=&quot;token comment&quot;&gt;# Size of text chunks&lt;/span&gt;
CHUNK_OVERLAP &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;50&lt;/span&gt;        &lt;span class=&quot;token comment&quot;&gt;# Overlap between chunks&lt;/span&gt;
TOP_K &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;                 &lt;span class=&quot;token comment&quot;&gt;# Number of chunks to retrieve&lt;/span&gt;
SIMILARITY_THRESHOLD &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.7&lt;/span&gt;  &lt;span class=&quot;token comment&quot;&gt;# Minimum similarity score&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
</content>
  </entry>
  <entry>
    <title>Finding Semantically Similar Content</title>
    <link href="https://shealy2020.github.io/posts/refactoring-content/" />
    <updated>2025-10-09T00:00:00Z</updated>
    <id>https://shealy2020.github.io/posts/refactoring-content/</id>
    <content type="html">&lt;p&gt;A common problem with technical documentation management is that documentation teams tend to have a lot of stressed-out contributors working on shared content, scattered across multiple departments and revised over an extended period. This is a recipe for content bloat and redundancy.&lt;/p&gt;
&lt;p&gt;All content needs some degree of custodial refactoring. Often refactoring takes the form of &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_deduplication&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;deduplication&lt;/a&gt; processes or merging similar content into a single source of truth &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_source_of_truth&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;(SSOT)&lt;/a&gt;.&lt;/p&gt;
&lt;!-- extra content needed about the importance of SSOT in LLMs --&gt;
&lt;p&gt;Eventually, I&#39;d like to contain a pool of content in a local LLM for RAG purposes. In taking baby steps toward that goal, I came across txtai and put together a prototype that surfaces semantically same or similar chunks in the form of a report. using a simple Markdown file as input.&lt;/p&gt;
&lt;p&gt;The processing is straightforward:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;preprocess_markdown.py&lt;/strong&gt; - Chunks MD by heading to populate a JSON file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;index_chunks.py&lt;/strong&gt; - txtai (sentence-transformers/all-MiniLM-L6-v2) indexes JSON file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;report_similarity.py&lt;/strong&gt; - txtai compares paragraph vectors. Reports similarity clusters and vector scores.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;GitHub repo here: &lt;a href=&quot;https://github.com/shealy2020/single-md-txtai-report&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/shealy2020/single-md-txtai-report&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I set out to use the hierarchical structure of the Markdown file as a vector dimension contributing to vector scores, but I couldn&#39;t figure out how to do that. Instead, I fell back to just comparing paragraph content under headings, so I might as well have compared blocks of text in a *.txt file.&lt;/p&gt;
&lt;p&gt;My goal is to leverage what I&#39;ve learned about comparing vectorized MD chunks and then apply it to &lt;a href=&quot;https://en.wikipedia.org/wiki/Darwin_Information_Typing_Architecture&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DITA&lt;/a&gt;, which is a highly structured authoring XML format with a lot of tagging and other metadata that could be helpful for vector searches.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>LLM Prompt Organizer</title>
    <link href="https://shealy2020.github.io/posts/llm-prompt-org/" />
    <updated>2025-09-20T00:00:00Z</updated>
    <id>https://shealy2020.github.io/posts/llm-prompt-org/</id>
    <content type="html">&lt;p&gt;I use the free versions of LLMs like ChatGPT, Claude, and Gemini that impose limits on a per session basis. LLMs calculate these limits through some combination of request counts and &lt;a href=&quot;https://shealy2020.github.io/posts/prompt-efficiencies/#tokens&quot;&gt;token use&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In an earlier post, I provided some general guidelines on how to increase &lt;a href=&quot;https://shealy2020.github.io/posts/prompt-efficiencies/&quot;&gt;prompt efficiencies&lt;/a&gt; by conserving tokens. For this project, I take &amp;quot;token conservation&amp;quot; further by building a tool that helps to organize the initial LLM prompt, then it generates a JSON file as input for the LLM. The tool reduces the overall number of input tokens by streamlining the query into a clear set of prompt instructions.&lt;/p&gt;
&lt;p&gt;Initially, I used highly structured XML as the output container but thought better of it. Instead, I went with semi-structured JSON because its syntax uses fewer characters, thus fewer tokens. Also, this project gave me another opportunity to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Vibe_coding&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;vibe coding&lt;/a&gt; as an aid in building this prompt tool, which I&#39;ll write about in a future post.&lt;/p&gt;
&lt;p&gt;For your own use, you&#39;ll find the &lt;a href=&quot;https://github.com/shealy2020/llm-prompt-organizer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;LLM Prompt Organizer on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;llm-session-limits&quot;&gt;LLM Session Limits&lt;/h2&gt;
&lt;p&gt;I use the freely available versions of ChatGPT, Claude, and Gemini. The table below contains the approximate limitation criteria for non-subscription users.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Service&lt;/th&gt;
&lt;th&gt;Key Usage Limits&lt;/th&gt;
&lt;th&gt;Approx. Token / Context Window&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Limited GPT-4o usage per ~5-hour window.&lt;/td&gt;
&lt;td&gt;~128k tokens (input + output).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Claude&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Message/session limit, resets ~every 5 hrs.&lt;/td&gt;
&lt;td&gt;~200k tokens (input + output).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Gemini&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;~2 req/min, ~50 req/day (API); ~5 prompts/day (app).&lt;/td&gt;
&lt;td&gt;Up to ~1M input / 65K output tokens.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;why-use-this-tool&quot;&gt;Why Use This Tool?&lt;/h2&gt;
&lt;h3 id=&quot;the-problem-with-unstructured-prompts&quot;&gt;The Problem with Unstructured Prompts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Important context gets buried in long text blocks.&lt;/li&gt;
&lt;li&gt;Reusing successful prompt patterns requires copy-pasting and manual editing.&lt;/li&gt;
&lt;li&gt;No clear separation between instructions, context, constraints, and data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;the-solution-semi-structured-prompts&quot;&gt;The Solution: Semi-structured Prompts&lt;/h3&gt;
&lt;p&gt;The LLM Prompt Organizer helps you create reusable prompts by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Separating concerns&lt;/strong&gt; - Breaks your prompt into logical components (role, context, task, format, etc.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ensuring completeness&lt;/strong&gt; - Visual interface reminds you of all the elements that make prompts effective&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enabling reusability&lt;/strong&gt; - Saves JSON files as templates for similar tasks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improving clarity&lt;/strong&gt; - JSON&#39;s semi-structured format helps the LLM understand the request.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supporting iteration&lt;/strong&gt; - Users can easily modify specific sections without rewriting everything.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;quick-start&quot;&gt;Quick Start&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download the &lt;code&gt;prompt-form-output json-1.html&lt;/code&gt; file from &lt;a href=&quot;https://github.com/shealy2020/llm-prompt-organizer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Open it in a web browser.&lt;/li&gt;
&lt;li&gt;Deactivate any fields irrelevant to your prompt.&lt;br&gt;
or&lt;br&gt;
Import an existing JSON prompt file.&lt;/li&gt;
&lt;li&gt;Add custom fields (Optional).&lt;/li&gt;
&lt;li&gt;Enter your content in the active fields.&lt;/li&gt;
&lt;li&gt;Export your prompt by clicking &amp;quot;Download JSON&amp;quot; or &amp;quot;Copy JSON&amp;quot;.&lt;/li&gt;
&lt;li&gt;Upload the JSON file directly or paste the content into the chat interface.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;json-output-sample&quot;&gt;JSON Output Sample&lt;/h2&gt;
&lt;p&gt;Your generated JSON will be similar to this:&lt;/p&gt;
&lt;pre class=&quot;language-json&quot; tabindex=&quot;0&quot;&gt;&lt;code class=&quot;language-json&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;&quot;instructions&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Process and respond to this prompt.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;You are an experienced data analyst specializing in retail performance.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;&quot;context&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;You are analyzing Q4 2024 sales data for a mid-sized retailer operating in North America. The dataset includes metrics such as total revenue, number of transactions, and product category breakdowns.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;&quot;task&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Identify the top three sales trends and provide insight into what factors may have contributed to these patterns.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;&quot;format&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Present your findings as three concise bullet points, each with one supporting sentence.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;&quot;examples&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;- Example Trend: Seasonal spike in apparel sales due to holiday promotions.&#92;n- Example Trend: Increased online sales driven by targeted email campaigns.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;&quot;constraints&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Focus only on quantitative insights supported by data. Avoid speculation not backed by sales metrics.&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;region,sales,transactions North America,1200000,32000 Europe,900000,28000 Asia,1100000,30000&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;&quot;audience&quot;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Retail operations managers looking for actionable insights to guide Q1 2025 planning.&quot;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;instructions&lt;/code&gt; key/value is automatically included and tells the LLM how to process the JSON content.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Prompt Efficiencies</title>
    <link href="https://shealy2020.github.io/posts/prompt-efficiencies/" />
    <updated>2025-08-15T00:00:00Z</updated>
    <id>https://shealy2020.github.io/posts/prompt-efficiencies/</id>
    <content type="html">&lt;h2 id=&quot;tokens&quot;&gt;Tokens&lt;/h2&gt;
&lt;p&gt;A token is a small chunk of text that the AI model uses to process your message and generate a reply. Both what you type and what the model sends back count toward your token limit. When you write short, focused prompts and ask for concise answers, you use fewer tokens overall. That means your sessions can last longer, responses come faster, and you’re less likely to hit token limits or get cut off mid-reply.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conserve tokens = longer sessions + faster responses + fewer cutoffs&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;1-prompt-design&quot;&gt;1️⃣ Prompt Design&lt;/h2&gt;
&lt;p&gt;✅ &lt;strong&gt;Do&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep prompts concise: use direct verbs (“summarize,” “revise,” “explain briefly”).&lt;/li&gt;
&lt;li&gt;Reference earlier text instead of re-pasting it.&lt;/li&gt;
&lt;li&gt;State the &lt;em&gt;output size&lt;/em&gt;:
&lt;ul&gt;
&lt;li&gt;“In 3 bullet points.”&lt;/li&gt;
&lt;li&gt;“Under 100 words.”&lt;/li&gt;
&lt;li&gt;“Code only—no explanation.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;🚫 &lt;strong&gt;Don’t&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Repeat long setup text (“As I mentioned earlier…”).&lt;/li&gt;
&lt;li&gt;Ask for multiple tasks in one prompt if they can be done sequentially.&lt;/li&gt;
&lt;li&gt;Use filler (“Could you please kindly explain…”).&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;2-conversation-management&quot;&gt;2️⃣ Conversation Management&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stay in the same chat thread so context persists.&lt;/li&gt;
&lt;li&gt;Split long projects:
&lt;ol&gt;
&lt;li&gt;Outline&lt;/li&gt;
&lt;li&gt;Draft one part&lt;/li&gt;
&lt;li&gt;Revise iteratively&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Delete or summarize unneeded sections before continuing.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;3-handling-large-text&quot;&gt;3️⃣ Handling Large Text&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Paste &lt;strong&gt;only relevant sections&lt;/strong&gt; (“Here’s the last 30 lines”).&lt;/li&gt;
&lt;li&gt;Summarize before submission (“This 20-page doc covers X, Y, Z…”).&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;compressed summaries&lt;/strong&gt; or keywords when possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;4-service-specific-tips&quot;&gt;4️⃣ Service-Specific Tips&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Service&lt;/th&gt;
&lt;th&gt;Efficiency Tactics&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;GPT-4o’s tokenizer is efficient. Reuse previous messages and cap outputs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Claude&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Takes long input well — summarize documents instead of feeding full text.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Gemini&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Large window; main limit is requests/day. Keep prompts precise and output short.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&quot;5-output-control&quot;&gt;5️⃣ Output Control&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ask for &lt;strong&gt;structured formats&lt;/strong&gt; (Markdown, table, JSON).&lt;/li&gt;
&lt;li&gt;Set maximums:
&lt;ul&gt;
&lt;li&gt;“Explain in ≤5 sentences.”&lt;/li&gt;
&lt;li&gt;“Show first 10 lines only.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For code: “Return only the function definition.”&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;6-quick-math&quot;&gt;6️⃣ Quick Math&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Word Count&lt;/th&gt;
&lt;th&gt;≈ Tokens&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;75 words&lt;/td&gt;
&lt;td&gt;~100 tokens&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;500 words&lt;/td&gt;
&lt;td&gt;~650 tokens&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1,000 words&lt;/td&gt;
&lt;td&gt;~1,300 tokens&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Short, direct, scoped prompts → 3–5× token savings.&lt;/p&gt;
</content>
  </entry>
</feed>