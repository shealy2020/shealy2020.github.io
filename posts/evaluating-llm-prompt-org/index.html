<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Evaluating Prompt Organizer</title>
        <meta name="description" content="Evaluating tool for creating JSON prompts to conserve LLM tokens">
        <meta name="generator" content="Eleventy v3.1.2">
        
        
        <link rel="icon" href="/images/favicon.ico" type="image/x-icon">
        <link rel="stylesheet" href="/dist/tm_1tXeKDB.css">
        
        <script src="/assets/js/nav.js"></script>
    </head>
    <body>
        <a href="#main" id="skip-link" class="visually-hidden">Skip to main content</a>
        <header class="site-header">
            <div class="header-nav-container">
                <div class="logo-container">
                    <!-- <img src="/images/merge-logo-1.svg" alt="Site Logo" class="site-logo"> -->
                    <a href="/">
                        
                            <img src="..\..\images\merge-logo-1.svg" alt="Site Logo" class="site-logo">
                        
                    </a>
                </div>
                <nav class="site-nav">
                    <ul class="nav"><li class="nav-item">
                                    <a href="/" class="nav-link" data-dropdown-toggle="">
                                        Home
                                    </a></li><li class="nav-item">
                                    <a href="/posts/" class="nav-link" data-dropdown-toggle="">
                                        Posts
                                    </a></li><li class="nav-item">
                                    <a href="/resources/" class="nav-link" data-dropdown-toggle="">
                                        Resources
                                    </a></li><li class="nav-item has-children">
                                    <a href="/about/" class="nav-link" data-dropdown-toggle="">
                                        About
                                    </a><ul class="dropdown"><li>
                                                    <a href="/resume/" class="dropdown-link">
                                                        Resume
                                                    </a>
                                                </li><li>
                                                    <a href="/work/" class="dropdown-link">
                                                        Work
                                                    </a>
                                                </li><li>
                                                    <a href="/contact/" class="dropdown-link">
                                                        Contact
                                                    </a>
                                                </li></ul></li></ul>
                </nav>
            </div>
            <h1 id="evaluating-prompt-organizer">
                    Evaluating Prompt Organizer
            </h1>
            <p>
                    Evaluating tool for creating JSON prompts to conserve LLM tokens
            </p>
        </header></body>
    
</html><main id="main">
<heading-anchors>
    



<p>Posted on: <time datetime="2025-12-02">02 December 2025</time></p>

<p>A few months ago, I created an <a href="/posts/llm-prompt-org/">LLM Prompt Organizer</a> that reduces token counts while providing a framework for a more disciplined approach to <a href="/posts/rag-terms-glossary/#prompt">prompt</a> engineering. It's simply an HTML form that provides default prompt elements such as context, constraints, audience, etc. Users can disable elements or add custom ones. Once users populate the form fields, they can drop the JSON prompt into the LLM of their choice.</p>
<p><img src="..\..\images\prompt-organizer-form-1.png" alt="Prompt Organizer" title="Prompt Organizer"></p>
<p>Was this JSON approach to prompt construction effective for overall token conservation?</p>
<p>After several months of use, I give it a strong <em>maybe</em>, with caveats...</p>
<p>It stands to reason that concise prompts pare down the number of request tokens, leading to longer sessions with fewer cutoffs than by submitting requests in paragraph form. I wasn't sure if an LLM used fewer tokens in its <em>response</em>, so I ran an ad hoc, not-so-scientific test in Gemini to quantify the total token delta for both JSON vs. paragraph formatting.</p>
<p>Here's the JSON prompt and a semantically similar paragraph, submitted to Gemini for the token comparison:</p>
<h4 id="json">JSON</h4>
<pre class="language-json" tabindex="0"><code class="language-json"><span class="token punctuation">{</span> 
<span class="token property">"instructions"</span><span class="token operator">:</span> <span class="token string">"Process and respond to this prompt."</span><span class="token punctuation">,</span> 
<span class="token property">"role"</span><span class="token operator">:</span> <span class="token string">"Computer science educator for AI concepts"</span><span class="token punctuation">,</span> 
<span class="token property">"context"</span><span class="token operator">:</span> <span class="token string">"User seeks basic transformer model understanding"</span><span class="token punctuation">,</span> 
<span class="token property">"task"</span><span class="token operator">:</span> <span class="token string">"Explain architecture, key components, importance for LLMs"</span><span class="token punctuation">,</span> 
<span class="token property">"format"</span><span class="token operator">:</span> <span class="token string">"Introduction, three main components, brief conclusion"</span><span class="token punctuation">,</span> 
<span class="token property">"examples"</span><span class="token operator">:</span> <span class="token string">"Attention mechanism like highlighting key words for context"</span><span class="token punctuation">,</span> 
<span class="token property">"constraints"</span><span class="token operator">:</span> <span class="token string">"No heavy math, use analogies, under 400 words"</span><span class="token punctuation">,</span> 
<span class="token property">"audience"</span><span class="token operator">:</span> <span class="token string">"Basic tech literacy, no ML background"</span> 
<span class="token punctuation">}</span></code></pre>
<h4 id="paragraph">Paragraph</h4>
<blockquote>
<p>I'm a computer science educator who specializes in making complex AI concepts accessible to beginners.
This explanation is intended for someone with basic tech literacy but no deep machine learning background.
Explain the transformer architecture, its key components, and why it's important for LLMs.
Structure your response with an introduction, followed by three main components
(attention mechanism, architecture overview, and practical significance), and end
with a brief conclusion. Keep the explanation clear, avoid heavy mathematical notation,
and limit your response to under 400 words. Use analogies to illustrate
concepts, for example, think of the attention mechanism like highlighting important words in a sentence to understand context better.</p>
</blockquote>
<p>I used <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener">OpenAI's Tokenizer</a> to generate token counts for the initial prompt request and its response and ran the tokenizer for both the JSON structure and the paragraph version, where one token corresponds to approximately four characters of text.</p>
<p><img src="..\..\images\openai-tokenizer-1.png" alt="OpenAI's Tokenizer Tool" title="OpenAI's Tokenizer Tool"></p>
<h3 id="results">Results</h3>
<table border="1">
  <thead>
    <tr>
      <th rowspan="2"></th>
      <th colspan="2">JSON</th>
      <th colspan="2">Paragraph</th>
    </tr>
    <tr>
      <th>Chars</th>
      <th>Tokens</th>
      <th>Chars</th>
      <th>Tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Request</strong></td>
      <td>522</td>
      <td>114</td>
      <td>853</td>
      <td>143</td>
    </tr>
    <tr>
      <td><strong>Response</strong></td>
      <td>3006</td>
      <td>585</td>
      <td>3460</td>
      <td>671</td>
    </tr>
    <tr>
      <td><strong>Total<br>Tokens</strong></td>
      <td></td>
      <td>699</td>
      <td></td>
      <td>814</td>
    </tr>
  </tbody>
</table>
<p>The JSON request and subsequent LLM response used a total of 115 fewer tokens than did its natural language counterpart. Rate limits are based, in part, by tokens per minute (TPM) use, so any token efficiencies are likely to length LLM sessions.</p>
<h3 id="postmortem">Postmortem</h3>
<p>A fatal flaw of my test, and with counting tokens in general, is that I was not able to quantify other factors contributing to the duration of an LLM session, such as computational resources, used and how these other factors are weighted. Moreover, each LLM has its own algorithm for this purpose. One of the principle downsides of the organizer itself is that, in trying to conserve tokens, the tool kneecaps a key strength of LLMs by favoring JSON-formatted prompts over natural language ones. Also, the organizer is overkill for simple prompts like 'Is Brookings Oregon really in a &quot;banana belt,&quot; and what's a banana belt anyway?'</p>
<p>A virtuous byproduct of the LLM Prompt Organizer, as its name implies, is that it provides a framework to assemble prompts in a consistent, repeatable way, while flexible enough to accommodate custom elements. The tool also helps to tighten up the language within the prompt to avoid ambiguity and serves as a solid base for follow-up prompts.</p>
<p>In sum, the prompt organizer is a blunt tool for gaming the rate limits of LLMs. One the other hand, it still holds up as a prompt templating tool, which was my primary reason for creating it.</p>
<p>Currently, users must copy JSON-formatted prompts into their LLM manually. If I were to improve the tool, I'd pass the JSON prompt and other necessary parameters to the LLM API and handle the response received from the API within the prompt organizer.</p>

<hr>
<p>More posts:</p>
<ul><li>Previous: <a href="/posts/colab-gdrive-rag-ui/">Gemini RAG Pipeline (Improved)</a></li>
</ul>


</heading-anchors></main><footer>

<p>
    <em>Built with
        <a href="https://www.11ty.dev/">Eleventy v3.1.2</a>
    </em>
</p></footer><!-- This page `/posts/evaluating-llm-prompt-org/` was built on 2025-12-04T17:18:57.615Z --><script type="module" src="/dist/xbxy_EL6cU.js"></script>