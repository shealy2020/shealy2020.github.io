<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Setting up Local RAG Pipeline in Docker</title>
        <meta name="description" content="Runs local RAG pipeline against filesystem documents within Docker">
        <meta name="generator" content="Eleventy v3.1.2">
        
        
        <link rel="icon" href="/images/favicon.ico" type="image/x-icon">
        <link rel="stylesheet" href="/dist/tm_1tXeKDB.css">
        
        <script src="/assets/js/nav.js"></script>
    </head>
    <body>
        <a href="#main" id="skip-link" class="visually-hidden">Skip to main content</a>
        <header class="site-header">
            <div class="header-nav-container">
                <div class="logo-container">
                    <!-- <img src="/images/merge-logo-1.svg" alt="Site Logo" class="site-logo"> -->
                    <a href="/">
                        
                            <img src="..\..\images\merge-logo-1.svg" alt="Site Logo" class="site-logo">
                        
                    </a>
                </div>
                <nav class="site-nav">
                    <ul class="nav"><li class="nav-item">
                                    <a href="/" class="nav-link" data-dropdown-toggle="">
                                        Home
                                    </a></li><li class="nav-item">
                                    <a href="/posts/" class="nav-link" data-dropdown-toggle="">
                                        Posts
                                    </a></li><li class="nav-item">
                                    <a href="/resources/" class="nav-link" data-dropdown-toggle="">
                                        Resources
                                    </a></li><li class="nav-item has-children">
                                    <a href="/about/" class="nav-link" data-dropdown-toggle="">
                                        About
                                    </a><ul class="dropdown"><li>
                                                    <a href="/resume/" class="dropdown-link">
                                                        Resume
                                                    </a>
                                                </li><li>
                                                    <a href="/work/" class="dropdown-link">
                                                        Work
                                                    </a>
                                                </li><li>
                                                    <a href="/contact/" class="dropdown-link">
                                                        Contact
                                                    </a>
                                                </li></ul></li></ul>
                </nav>
            </div>
            <h1 id="setting-up-local-rag-pipeline-in-docker">
                    Setting up Local RAG Pipeline in Docker
            </h1>
            <p>
                    Runs local RAG pipeline against filesystem documents within Docker
            </p>
        </header></body>
    
</html><main id="main">
<heading-anchors>
    



<p>Posted on: <time datetime="2025-10-27">27 October 2025</time></p>

<p>This project is a prototype of a local Retrieval-Augmented Generation (RAG) system, exploring how RAG pipelines work. It's a useful step before scaling or adapting it to production environments, where proprietary information is behind a firewall. Everything runs entirely on a local pc and requires no cloud services or external data transfers. I used open-source tools only (Chroma, Sentence Transformers, and Ollama) with Python scripting.</p>
<p>You'll find the package on <a href="https://github.com/shealy2020/rag-local" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Windows 11 with WSL 2</li>
<li>Docker Desktop installed with WSL enabled</li>
<li>Python installed</li>
<li>At least 4GB RAM available</li>
<li>At least 5GB disk space</li>
</ul>
<h2 id="rag-pipeline-components">RAG Pipeline Components</h2>
<ul>
<li><strong>LLM</strong>: Gemma 2B (via Ollama)</li>
<li><strong>Embeddings</strong>: all-MiniLM-L6-v2</li>
<li><strong>Vector Store</strong>: ChromaDB</li>
<li><strong>Interface</strong>: Flask + HTML</li>
<li><strong>Supported Formats</strong>: Markdown (.md), HTML (.html, .htm)</li>
</ul>
<h2 id="privacy-and-security">Privacy &amp; Security</h2>
<p>This prototype provides basic safeguards but is not production ready.</p>
<ul>
<li>All data stays on your machine.</li>
<li>No external API calls after setup.</li>
<li>No telemetry or analytics.</li>
<li>Can run with complete network isolation.</li>
<li>Docker container isolation.</li>
<li>API Key for extra layer of security.</li>
</ul>
<h2 id="summary-of-python-scripts">Summary of Python Scripts</h2>
<h3 id="1-config-py-configuration-management"><strong>1. config.py</strong> - Configuration Management</h3>
<p>Central configuration file that defines all system settings:</p>
<ul>
<li><strong>Paths</strong>: Input documents, ChromaDB storage, model cache locations.</li>
<li><strong>Models</strong>: Uses <code>sentence-transformers/all-MiniLM-L6-v2</code> for embeddings and <code>gemma:2b</code> via Ollama for text generation.</li>
<li><strong>Chunking</strong>: Documents split into 512-character chunks with 50-character overlap.</li>
<li><strong>Retrieval</strong>: Retrieves top 5 most similar chunks with 0.7 similarity threshold.</li>
<li><strong>Server</strong>: Flask runs on port 5000.</li>
<li>Uses environment variables for flexibility in deployment.</li>
</ul>
<hr>
<h3 id="2-ingest-py-document-processing-and-indexing"><strong>2. ingest.py</strong> - Document Processing &amp; Indexing</h3>
<p>Handles the ingestion pipeline that prepares documents for retrieval:</p>
<p><strong>DocumentProcessor class:</strong></p>
<ul>
<li>Reads Markdown and HTML files.</li>
<li>Converts them to plain text (strips formatting, scripts, styles).</li>
<li>Splits text into overlapping chunks for better context preservation.</li>
<li>Creates unique IDs for each chunk using MD5 hashing.</li>
</ul>
<p><strong>VectorStore class:</strong></p>
<ul>
<li>Manages ChromaDB persistent storage.</li>
<li>Generates embeddings using SentenceTransformer.</li>
<li>Stores document chunks with their vector embeddings.</li>
<li>Provides collection statistics and reset functionality.</li>
</ul>
<p><strong>Main workflow:</strong></p>
<ul>
<li>Scans input directory for <code>.md</code>, <code>.html</code>, <code>.htm</code> files</li>
<li>Processes each file into chunks.</li>
<li>Generates embeddings in batches (100 at a time).</li>
<li>Stores everything in ChromaDB for efficient similarity search.</li>
</ul>
<hr>
<h3 id="3-query-py-query-processing-engine"><strong>3. query.py</strong> - Query Processing Engine</h3>
<p>The core RAG logic that answers user questions:</p>
<p><strong>RAGEngine class:</strong></p>
<ul>
<li><strong>Retrieval</strong>: Converts user questions to embeddings and finds similar document chunks using vector search.</li>
<li><strong>Context Building</strong>: Assembles retrieved chunks with source attribution.</li>
<li><strong>Response Generation</strong>: Uses Ollama LLM with a structured prompt that instructs it to answer based only on provided context.</li>
<li><strong>Error Handling</strong>: Handles missing collections or empty databases.</li>
</ul>
<p><strong>Query flow:</strong></p>
<ol>
<li>Embeds the user's question.</li>
<li>Finds top-K most similar document chunks.</li>
<li>Builds context from retrieved chunks.</li>
<li>Sends context + question to LLM.</li>
<li>Returns answer with source citations.</li>
</ol>
<hr>
<h3 id="4-server-py-web-api-server"><strong>4. server.py</strong> - Web API Server</h3>
<p>Flask-based web server that exposes the RAG system via HTTP:</p>
<p><strong>Features:</strong></p>
<ul>
<li><strong>API Key Authentication</strong>: Optional security via <code>X-API-Key</code> header (controlled by <code>RAG_API_KEY</code> environment variable).</li>
<li><strong>Three endpoints:</strong>
<ul>
<li><code>GET /</code> - Serves the web interface.</li>
<li><code>GET /api/status</code> - Returns system health (e.g., document count, available files, readiness).</li>
<li><code>POST /api/query</code> - Processes questions and returns AI-generated answers.</li>
</ul>
</li>
<li><strong>Lazy Loading</strong>: RAG engine initializes on first query for faster startup.</li>
<li><strong>Error Handling</strong>: Error responses with HTTP status codes.</li>
<li><strong>Authentication decorator</strong> ensures protected endpoints require valid API keys when enabled.</li>
</ul>
<hr>
<h3 id="system-architecture">System Architecture</h3>
<pre><code>User Question → Flask Server → RAG Engine → ChromaDB (Vector Search)
                                    ↓
                            Retrieved Chunks
                                    ↓
                            Ollama LLM (gemma:2b)
                                    ↓
                            Answer + Sources → User
</code></pre>
<h2 id="installation-and-setup">Installation &amp; Setup</h2>
<ol>
<li>
<p>Start Docker Desktop in Windows.</p>
</li>
<li>
<p>Start WSL. A Bash terminal opens in your home directory.</p>
</li>
<li>
<p>Copy <em>rag-local</em> directory to your home directory.</p>
<p>Project Structure:</p>
<pre><code>rag-local/
├── Dockerfile              # Container definition
├── docker-compose.yml      # Docker Compose configuration
├── requirements.txt        # Python dependencies
├── README.md              # This file
├── app/
│   ├── config.py          # Configuration settings
│   ├── ingest.py          # Document ingestion
│   ├── query.py           # RAG query engine
│   ├── server.py          # Flask web server
│   ├── templates/
│   │   └── index.html     # Web interface
│   └── static/
│       └── style.css      # CSS styling
└── data/
 ├── input/             # Your documents (add files here)
 ├── chroma_db/         # Vector database (auto-created)
 └── models/            # Cached ML models (auto-created)
</code></pre>
<p>The project contains a sample <code>.md</code> and <code>.html</code> file in the <em>data/input</em> directory.</p>
</li>
<li>
<p>Generate an API Key.</p>
</li>
</ol>
<pre class="language-bash" tabindex="0"><code class="language-bash">openssl rand <span class="token parameter variable">-base64</span> <span class="token number">32</span></code></pre>
<p>This key will add a minimal security layer.</p>
<ol start="5">
<li>
<p>Copy the API Key. You will need it for the steps that follow. (Also, retain it for later use.)</p>
</li>
<li>
<p>Open <em>docker-compose.yml</em> and find the following line:
<em>RAG_API_KEY=change-this-to-a-strong-random-key</em></p>
</li>
<li>
<p>Replace &quot;change-this-to-a-strong-random-key&quot; with your API key.</p>
</li>
<li>
<p>Navigate to the project directory.</p>
</li>
</ol>
<pre class="language-bash" tabindex="0"><code class="language-bash"><span class="token builtin class-name">cd</span> ~/rag-local</code></pre>
<ol start="9">
<li>Build the Docker image.</li>
</ol>
<pre class="language-bash" tabindex="0"><code class="language-bash"><span class="token function">docker-compose</span> build</code></pre>
<p>​	This may take 10-20 minutes as it downloads all required components.</p>
<ol start="10">
<li>Start the container.</li>
</ol>
<pre class="language-bash" tabindex="0"><code class="language-bash"><span class="token function">docker-compose</span> up <span class="token parameter variable">-d</span></code></pre>
<p>​	The first startup after a build may take 5-10 minutes as it downloads the Gemma 2B model (~1.7GB).</p>
<ol start="11">
<li>Open a browser and enter:</li>
</ol>
<pre><code>http://localhost:5000
</code></pre>
<ol start="12">
<li>
<p>At the prompt, enter the API key you copied earlier, then click <strong>OK</strong>.</p>
<p>The RAG interface opens (Flask + HTML) where you can query against the Markdown and HTML input files.</p>
<p>The interface also provides commands to add and update input files.</p>
</li>
</ol>
<h2 id="useful-docker-commands">Useful Docker Commands</h2>
<pre class="language-bash" tabindex="0"><code class="language-bash"><span class="token comment"># Start the container</span>
<span class="token function">docker-compose</span> up

<span class="token comment"># Start in background (-d = detached mode)</span>
<span class="token function">docker-compose</span> up <span class="token parameter variable">-d</span>

<span class="token comment"># Stop the container</span>
<span class="token function">docker-compose</span> down

<span class="token comment"># View logs (-f = follow)</span>
<span class="token function">docker-compose</span> logs <span class="token parameter variable">-f</span>

<span class="token comment"># Rebuild after code changes</span>
<span class="token function">docker-compose</span> build --no-cache
</code></pre>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="container-wont-start">Container won't start</h3>
<ul>
<li>Check to verify that Docker Desktop is running.</li>
<li>In Docker Desktop, go to <strong>Settings</strong> &gt; <strong>General</strong>. Verify that <em>Use the WSL 2 based engine</em> is enabled.</li>
<li>Check port 5000 is not in use: <code>netstat -an | grep 5000</code>.</li>
<li>View logs: <code>docker-compose logs</code>.</li>
</ul>
<h3 id="ollama-errors">Ollama errors</h3>
<ul>
<li>Wait longer on first startup (model download takes time).</li>
<li>Check container logs: <code>docker-compose logs | grep ollama</code>.</li>
<li>Restart container: <code>docker-compose restart</code>.</li>
</ul>
<h3 id="out-of-memory">Out of memory</h3>
<ul>
<li>Reduce <code>TOP_K</code> in <code>app/config.py</code> (default: 5).</li>
<li>Reduce <code>CHUNK_SIZE</code> in <code>app/config.py</code> (default: 512).</li>
<li>Close other applications.</li>
<li>Restart Docker Desktop.</li>
</ul>
<h3 id="slow-responses">Slow responses</h3>
<ul>
<li>This is normal for CPU-only inference with limited pc resources.</li>
<li>Gemma 2B on CPU typically takes 10-30 seconds per response.</li>
<li>Consider reducing the amount of context retrieved (TOP_K).</li>
</ul>
<h2 id="configuration">Configuration</h2>
<p>Edit <code>app/config.py</code> to customize:</p>
<pre class="language-python" tabindex="0"><code class="language-python">CHUNK_SIZE <span class="token operator">=</span> <span class="token number">512</span>          <span class="token comment"># Size of text chunks</span>
CHUNK_OVERLAP <span class="token operator">=</span> <span class="token number">50</span>        <span class="token comment"># Overlap between chunks</span>
TOP_K <span class="token operator">=</span> <span class="token number">5</span>                 <span class="token comment"># Number of chunks to retrieve</span>
SIMILARITY_THRESHOLD <span class="token operator">=</span> <span class="token number">0.7</span>  <span class="token comment"># Minimum similarity score</span></code></pre>

<hr>
<p>More posts:</p>
<ul><li>Next: <a href="/posts/doc-gen-rag-pipeline/">Generating Information Products with RAG</a></li><li>Previous: <a href="/posts/finding-similar-content/">Finding Semantically Similar Content</a></li>
</ul>


</heading-anchors></main><footer>

<p>
    <em>Built with
        <a href="https://www.11ty.dev/">Eleventy v3.1.2</a>
    </em>
</p></footer><!-- This page `/posts/rag-in-docker/` was built on 2025-12-15T20:36:27.644Z --><script type="module" src="/dist/xbxy_EL6cU.js"></script>