<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Setting up Local RAG pipeline in Docker</title>
        <meta name="description" content="Runs local RAG pipeline to query filesystem documents within a Docker container on WSL 2">
        <meta name="generator" content="Eleventy v3.1.2">
        
        
    <link rel="stylesheet" href="/dist/G_x5EZb90o.css">
    
    <script src="../assets/js/nav.js"></script>
</head>
<body>
    <a href="#main" id="skip-link" class="visually-hidden">Skip to main content</a>
    <header class="site-header">
        <nav class="site-nav">
            <ul class="nav"><li class="nav-item">
                            <a href="/" class="nav-link" data-dropdown-toggle="">
                                Home
                            </a></li><li class="nav-item">
                            <a href="/posts/" class="nav-link" data-dropdown-toggle="">
                                Posts
                            </a></li><li class="nav-item">
                            <a href="/resources/" class="nav-link" data-dropdown-toggle="">
                                Resources
                            </a></li><li class="nav-item has-children">
                            <a href="/about/" class="nav-link" data-dropdown-toggle="">
                                About
                            </a><ul class="dropdown"><li>
                                            <a href="/resume/" class="dropdown-link">
                                                Resume
                                            </a>
                                        </li><li>
                                            <a href="/work/" class="dropdown-link">
                                                Work
                                            </a>
                                        </li><li>
                                            <a href="/contact/" class="dropdown-link">
                                                Contact
                                            </a>
                                        </li></ul></li></ul>
        </nav>
        <h1 id="setting-up-local-rag-pipeline-in-docker">
                Setting up Local RAG pipeline in Docker
        </h1>
        <p>
                Runs local RAG pipeline to query filesystem documents within a Docker container on WSL 2
        </p>
    </header>
    <main id="main">
        <heading-anchors>
            



<p>Posted on: <time datetime="2025-10-27">27 October 2025</time></p>

<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Windows 11 with WSL 2</li>
<li>Docker Desktop installed with WSL enabled</li>
<li>At least 4GB RAM available</li>
<li>At least 5GB disk space</li>
</ul>
<h2 id="rag-pipeline-components">RAG Pipeline Components</h2>
<ul>
<li><strong>LLM</strong>: Gemma 2B (via Ollama)</li>
<li><strong>Embeddings</strong>: all-MiniLM-L6-v2</li>
<li><strong>Vector Store</strong>: ChromaDB</li>
<li><strong>Interface</strong>: Flask + HTML</li>
<li><strong>Supported Formats</strong>: Markdown (.md), HTML (.html, .htm)</li>
</ul>
<h2 id="privacy-and-security">Privacy &amp; Security</h2>
<p>This pipeline is a prototype for a larger implementation. It contains basic safeguards but is not production ready.</p>
<ul>
<li>All data stays on your machine</li>
<li>No external API calls after setup</li>
<li>No telemetry or analytics</li>
<li>Can run with complete network isolation</li>
<li>Docker container isolation</li>
<li>API Key for extra layer of security</li>
</ul>
<h2 id="summary-of-python-scripts">Summary of Python Scripts</h2>
<h3 id="1-config-py-configuration-management"><strong>1. config.py</strong> - Configuration Management</h3>
<p>Central configuration file that defines all system settings:</p>
<ul>
<li><strong>Paths</strong>: Input documents, ChromaDB storage, model cache locations</li>
<li><strong>Models</strong>: Uses <code>sentence-transformers/all-MiniLM-L6-v2</code> for embeddings and <code>gemma:2b</code> via Ollama for text generation</li>
<li><strong>Chunking</strong>: Documents split into 512-character chunks with 50-character overlap</li>
<li><strong>Retrieval</strong>: Retrieves top 5 most similar chunks with 0.7 similarity threshold</li>
<li><strong>Server</strong>: Flask runs on port 5000</li>
<li>Uses environment variables for flexibility in deployment</li>
</ul>
<hr>
<h3 id="2-ingest-py-document-processing-and-indexing"><strong>2. ingest.py</strong> - Document Processing &amp; Indexing</h3>
<p>Handles the ingestion pipeline that prepares documents for retrieval:</p>
<p><strong>DocumentProcessor class:</strong></p>
<ul>
<li>Reads Markdown and HTML files</li>
<li>Converts them to plain text (strips formatting, scripts, styles)</li>
<li>Splits text into overlapping chunks for better context preservation</li>
<li>Creates unique IDs for each chunk using MD5 hashing</li>
</ul>
<p><strong>VectorStore class:</strong></p>
<ul>
<li>Manages ChromaDB persistent storage</li>
<li>Generates embeddings using SentenceTransformer</li>
<li>Stores document chunks with their vector embeddings</li>
<li>Provides collection statistics and reset functionality</li>
</ul>
<p><strong>Main workflow:</strong></p>
<ul>
<li>Scans input directory for <code>.md</code>, <code>.html</code>, <code>.htm</code> files</li>
<li>Processes each file into chunks</li>
<li>Generates embeddings in batches (100 at a time)</li>
<li>Stores everything in ChromaDB for efficient similarity search</li>
</ul>
<hr>
<h3 id="3-query-py-query-processing-engine"><strong>3. query.py</strong> - Query Processing Engine</h3>
<p>The core RAG logic that answers user questions:</p>
<p><strong>RAGEngine class:</strong></p>
<ul>
<li><strong>Retrieval</strong>: Converts user questions to embeddings and finds similar document chunks using vector search</li>
<li><strong>Context Building</strong>: Assembles retrieved chunks with source attribution</li>
<li><strong>Response Generation</strong>: Uses Ollama LLM with a structured prompt that instructs it to answer based only on provided context</li>
<li><strong>Error Handling</strong>: Gracefully handles missing collections or empty databases</li>
</ul>
<p><strong>Query flow:</strong></p>
<ol>
<li>Embed the user's question</li>
<li>Find top-K most similar document chunks</li>
<li>Build context from retrieved chunks</li>
<li>Send context + question to LLM</li>
<li>Return answer with source citations</li>
</ol>
<hr>
<h3 id="4-server-py-web-api-server"><strong>4. server.py</strong> - Web API Server</h3>
<p>Flask-based web server that exposes the RAG system via HTTP:</p>
<p><strong>Features:</strong></p>
<ul>
<li><strong>API Key Authentication</strong>: Optional security via <code>X-API-Key</code> header (controlled by <code>RAG_API_KEY</code> environment variable)</li>
<li><strong>Three endpoints:</strong>
<ul>
<li><code>GET /</code> - Serves the web interface</li>
<li><code>GET /api/status</code> - Returns system health (document count, available files, readiness)</li>
<li><code>POST /api/query</code> - Processes questions and returns AI-generated answers</li>
</ul>
</li>
<li><strong>Lazy Loading</strong>: RAG engine initialized on first query for faster startup</li>
<li><strong>Error Handling</strong>: Comprehensive error responses with appropriate HTTP status codes</li>
</ul>
<p><strong>Authentication decorator</strong> ensures protected endpoints require valid API keys when enabled.</p>
<hr>
<h3 id="system-architecture">System Architecture</h3>
<pre><code>User Question → Flask Server → RAG Engine → ChromaDB (Vector Search)
                                    ↓
                            Retrieved Chunks
                                    ↓
                            Ollama LLM (gemma:2b)
                                    ↓
                            Answer + Sources → User
</code></pre>
<h2 id="installation-and-setup">Installation &amp; Setup</h2>
<ol>
<li>
<p>Start Docker Desktop in Windows.</p>
</li>
<li>
<p>Start WSL. A Bash terminal opens in your home directory.</p>
</li>
<li>
<p>Copy <em>rag-local</em> directory to your home directory.</p>
<p>Project Structure:</p>
<pre><code>rag-local/
├── Dockerfile              # Container definition
├── docker-compose.yml      # Docker Compose configuration
├── requirements.txt        # Python dependencies
├── README.md              # This file
├── app/
│   ├── config.py          # Configuration settings
│   ├── ingest.py          # Document ingestion
│   ├── query.py           # RAG query engine
│   ├── server.py          # Flask web server
│   ├── templates/
│   │   └── index.html     # Web interface
│   └── static/
│       └── style.css      # CSS styling
└── data/
 ├── input/             # Your documents (add files here)
 ├── chroma_db/         # Vector database (auto-created)
 └── models/            # Cached ML models (auto-created)
</code></pre>
<p>The project contains a sample <code>.md</code> and <code>.html</code> file in the <em>data/input</em> directory.</p>
</li>
<li>
<p>Generate an API Key.</p>
</li>
</ol>
<pre class="language-bash" tabindex="0"><code class="language-bash">openssl rand <span class="token parameter variable">-base64</span> <span class="token number">32</span></code></pre>
<p>This key will add a minimal security layer.</p>
<ol start="5">
<li>
<p>Copy the API Key. You will need it for the steps that follow. (Also, retain it for later use.)</p>
</li>
<li>
<p>Open <em>docker-compose.yml</em> and find the following line:
<em>RAG_API_KEY=change-this-to-a-strong-random-key</em></p>
</li>
<li>
<p>Replace &quot;change-this-to-a-strong-random-key&quot; with your API key.</p>
</li>
<li>
<p>Navigate to the project directory.</p>
</li>
</ol>
<pre class="language-bash" tabindex="0"><code class="language-bash"><span class="token builtin class-name">cd</span> ~/rag-local</code></pre>
<ol start="9">
<li>Build the Docker image.</li>
</ol>
<pre class="language-bash" tabindex="0"><code class="language-bash"><span class="token function">docker-compose</span> build</code></pre>
<p>​	This may take 10-20 minutes as it downloads all required components.</p>
<ol start="10">
<li>Start the container.</li>
</ol>
<pre class="language-bash" tabindex="0"><code class="language-bash"><span class="token function">docker-compose</span> up <span class="token parameter variable">-d</span></code></pre>
<p>​	The first startup after a build may take 5-10 minutes as it downloads the Gemma 2B model (~1.7GB).</p>
<ol start="11">
<li>Open a browser and enter:</li>
</ol>
<pre><code>http://localhost:5000
</code></pre>
<ol start="12">
<li>
<p>At the prompt, enter the API key you copied earlier, then click <strong>OK</strong>.</p>
<p>The RAG interface opens (Flask + HTML) where you can query against the Markdown and HTML input files.</p>
<p>The interface also provides commands to add and update input files.</p>
</li>
</ol>
<h2 id="useful-docker-commands">Useful Docker Commands</h2>
<pre class="language-bash" tabindex="0"><code class="language-bash"><span class="token comment"># Start the container</span>
<span class="token function">docker-compose</span> up

<span class="token comment"># Start in background (-d = detached mode)</span>
<span class="token function">docker-compose</span> up <span class="token parameter variable">-d</span>

<span class="token comment"># Stop the container</span>
<span class="token function">docker-compose</span> down

<span class="token comment"># View logs (-f = follow)</span>
<span class="token function">docker-compose</span> logs <span class="token parameter variable">-f</span>

<span class="token comment"># Rebuild after code changes</span>
<span class="token function">docker-compose</span> build --no-cache
</code></pre>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="container-wont-start">Container won't start</h3>
<ul>
<li>Check to verify that Docker Desktop is running</li>
<li>In Docker Desktop, go to <strong>Settings</strong> &gt; <strong>General</strong>. Verify that <em>Use the WSL 2 based engine</em> is enabled</li>
<li>Check port 5000 is not in use: <code>netstat -an | grep 5000</code></li>
<li>View logs: <code>docker-compose logs</code></li>
</ul>
<h3 id="ollama-errors">Ollama errors</h3>
<ul>
<li>Wait longer on first startup (model download takes time)</li>
<li>Check container logs: <code>docker-compose logs | grep ollama</code></li>
<li>Restart container: <code>docker-compose restart</code></li>
</ul>
<h3 id="out-of-memory">Out of memory</h3>
<ul>
<li>Reduce <code>TOP_K</code> in <code>app/config.py</code> (default: 5)</li>
<li>Reduce <code>CHUNK_SIZE</code> in <code>app/config.py</code> (default: 512)</li>
<li>Close other applications</li>
<li>Restart Docker Desktop</li>
</ul>
<h3 id="slow-responses">Slow responses</h3>
<ul>
<li>This is normal for CPU-only inference with limited pc resources</li>
<li>Gemma 2B on CPU typically takes 10-30 seconds per response</li>
<li>Consider reducing the amount of context retrieved (TOP_K)</li>
</ul>
<h2 id="configuration">Configuration</h2>
<p>Edit <code>app/config.py</code> to customize:</p>
<pre class="language-python" tabindex="0"><code class="language-python">CHUNK_SIZE <span class="token operator">=</span> <span class="token number">512</span>          <span class="token comment"># Size of text chunks</span>
CHUNK_OVERLAP <span class="token operator">=</span> <span class="token number">50</span>        <span class="token comment"># Overlap between chunks</span>
TOP_K <span class="token operator">=</span> <span class="token number">5</span>                 <span class="token comment"># Number of chunks to retrieve</span>
SIMILARITY_THRESHOLD <span class="token operator">=</span> <span class="token number">0.7</span>  <span class="token comment"># Minimum similarity score</span></code></pre>

<hr>
<p>More posts:</p>
<ul><li>Previous: <a href="/posts/refactoring-content/">Surfacing Semantically Similar Content</a></li>
</ul>


        </heading-anchors>
    </main>
    <footer>
        
        <p>
            <em>Built with
                <a href="https://www.11ty.dev/">Eleventy v3.1.2</a>
            </em>
        </p>
    </footer>
    <!-- This page `/posts/local-rag-in-docker/` was built on 2025-10-27T18:25:52.601Z -->
    <script type="module" src="/dist/xbxy_EL6cU.js"></script>
</body></html>